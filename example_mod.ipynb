{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@daniel820710/%E5%88%A9%E7%94%A8keras%E5%BB%BA%E6%A7%8Blstm%E6%A8%A1%E5%9E%8B-%E4%BB%A5stock-prediction-%E7%82%BA%E4%BE%8B-1-67456e0a0b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab/anaconda3/envs/keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.968700</td>\n",
       "      <td>43.968700</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>27.112253</td>\n",
       "      <td>1003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43.968700</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>43.968700</td>\n",
       "      <td>27.305082</td>\n",
       "      <td>480500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.218700</td>\n",
       "      <td>44.375000</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>27.362904</td>\n",
       "      <td>201300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44.406200</td>\n",
       "      <td>44.843700</td>\n",
       "      <td>44.375000</td>\n",
       "      <td>27.652185</td>\n",
       "      <td>529400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.968700</td>\n",
       "      <td>45.093700</td>\n",
       "      <td>44.468700</td>\n",
       "      <td>27.767893</td>\n",
       "      <td>531500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>44.968700</td>\n",
       "      <td>45.062500</td>\n",
       "      <td>44.718700</td>\n",
       "      <td>27.748577</td>\n",
       "      <td>492100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>44.968700</td>\n",
       "      <td>45.125000</td>\n",
       "      <td>44.906200</td>\n",
       "      <td>27.748577</td>\n",
       "      <td>596100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44.812500</td>\n",
       "      <td>44.812500</td>\n",
       "      <td>44.562500</td>\n",
       "      <td>27.555738</td>\n",
       "      <td>122100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>44.656200</td>\n",
       "      <td>44.750000</td>\n",
       "      <td>44.531200</td>\n",
       "      <td>27.594305</td>\n",
       "      <td>379600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44.781200</td>\n",
       "      <td>45.125000</td>\n",
       "      <td>44.781200</td>\n",
       "      <td>27.729336</td>\n",
       "      <td>19500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>44.875000</td>\n",
       "      <td>44.875000</td>\n",
       "      <td>44.593700</td>\n",
       "      <td>27.517174</td>\n",
       "      <td>42500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>44.468700</td>\n",
       "      <td>44.468700</td>\n",
       "      <td>43.406200</td>\n",
       "      <td>26.822969</td>\n",
       "      <td>374800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>43.468700</td>\n",
       "      <td>43.531200</td>\n",
       "      <td>43.281200</td>\n",
       "      <td>26.803724</td>\n",
       "      <td>210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>43.937500</td>\n",
       "      <td>43.937500</td>\n",
       "      <td>42.812500</td>\n",
       "      <td>26.784391</td>\n",
       "      <td>378100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>43.406200</td>\n",
       "      <td>43.562500</td>\n",
       "      <td>43.343700</td>\n",
       "      <td>26.880852</td>\n",
       "      <td>34900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>43.687500</td>\n",
       "      <td>43.781200</td>\n",
       "      <td>43.562500</td>\n",
       "      <td>26.977249</td>\n",
       "      <td>513600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>43.843700</td>\n",
       "      <td>43.875000</td>\n",
       "      <td>43.468700</td>\n",
       "      <td>26.958002</td>\n",
       "      <td>373700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>43.718700</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>43.718700</td>\n",
       "      <td>27.305082</td>\n",
       "      <td>26300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>44.218700</td>\n",
       "      <td>44.375000</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>27.362904</td>\n",
       "      <td>44500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>44.437500</td>\n",
       "      <td>44.437500</td>\n",
       "      <td>44.187500</td>\n",
       "      <td>27.401474</td>\n",
       "      <td>66200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>44.562500</td>\n",
       "      <td>44.562500</td>\n",
       "      <td>44.218700</td>\n",
       "      <td>27.324339</td>\n",
       "      <td>66500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>44.312500</td>\n",
       "      <td>44.937500</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>27.729336</td>\n",
       "      <td>182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.156200</td>\n",
       "      <td>44.937500</td>\n",
       "      <td>27.845016</td>\n",
       "      <td>280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>45.187500</td>\n",
       "      <td>45.187500</td>\n",
       "      <td>44.875000</td>\n",
       "      <td>27.690750</td>\n",
       "      <td>89500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>44.937500</td>\n",
       "      <td>45.125000</td>\n",
       "      <td>44.718700</td>\n",
       "      <td>27.613615</td>\n",
       "      <td>40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>44.843700</td>\n",
       "      <td>45.750000</td>\n",
       "      <td>44.843700</td>\n",
       "      <td>28.230696</td>\n",
       "      <td>50800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>45.656200</td>\n",
       "      <td>45.687500</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>28.134245</td>\n",
       "      <td>169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>45.593700</td>\n",
       "      <td>45.687500</td>\n",
       "      <td>45.406200</td>\n",
       "      <td>28.192129</td>\n",
       "      <td>194400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>45.718700</td>\n",
       "      <td>45.843700</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>28.114969</td>\n",
       "      <td>70900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>45.187500</td>\n",
       "      <td>45.218700</td>\n",
       "      <td>44.812500</td>\n",
       "      <td>27.825703</td>\n",
       "      <td>643600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6316</th>\n",
       "      <td>275.679993</td>\n",
       "      <td>276.190002</td>\n",
       "      <td>271.290009</td>\n",
       "      <td>268.143005</td>\n",
       "      <td>121907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6317</th>\n",
       "      <td>271.410004</td>\n",
       "      <td>273.170013</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>264.244049</td>\n",
       "      <td>176855100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6318</th>\n",
       "      <td>265.799988</td>\n",
       "      <td>269.720001</td>\n",
       "      <td>264.820007</td>\n",
       "      <td>265.606201</td>\n",
       "      <td>139083200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6319</th>\n",
       "      <td>267.730011</td>\n",
       "      <td>272.890015</td>\n",
       "      <td>267.609985</td>\n",
       "      <td>268.676025</td>\n",
       "      <td>97307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6320</th>\n",
       "      <td>273.299988</td>\n",
       "      <td>273.390015</td>\n",
       "      <td>271.179993</td>\n",
       "      <td>269.357147</td>\n",
       "      <td>79213200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6321</th>\n",
       "      <td>270.420013</td>\n",
       "      <td>273.179993</td>\n",
       "      <td>270.200012</td>\n",
       "      <td>269.258453</td>\n",
       "      <td>87063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6322</th>\n",
       "      <td>273.549988</td>\n",
       "      <td>274.239990</td>\n",
       "      <td>272.420013</td>\n",
       "      <td>270.561401</td>\n",
       "      <td>66901200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6323</th>\n",
       "      <td>275.700012</td>\n",
       "      <td>278.869995</td>\n",
       "      <td>275.339996</td>\n",
       "      <td>275.269775</td>\n",
       "      <td>113625300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6324</th>\n",
       "      <td>279.200012</td>\n",
       "      <td>279.910004</td>\n",
       "      <td>278.079987</td>\n",
       "      <td>274.924316</td>\n",
       "      <td>71924800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6325</th>\n",
       "      <td>279.839996</td>\n",
       "      <td>280.410004</td>\n",
       "      <td>276.029999</td>\n",
       "      <td>273.147583</td>\n",
       "      <td>91968900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6326</th>\n",
       "      <td>277.809998</td>\n",
       "      <td>278.019989</td>\n",
       "      <td>274.670013</td>\n",
       "      <td>271.745880</td>\n",
       "      <td>105895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6327</th>\n",
       "      <td>275.880005</td>\n",
       "      <td>276.609985</td>\n",
       "      <td>274.429993</td>\n",
       "      <td>271.449738</td>\n",
       "      <td>83433000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6328</th>\n",
       "      <td>274.500000</td>\n",
       "      <td>275.390015</td>\n",
       "      <td>274.140015</td>\n",
       "      <td>271.744141</td>\n",
       "      <td>100343700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6329</th>\n",
       "      <td>273.350006</td>\n",
       "      <td>274.399994</td>\n",
       "      <td>268.619995</td>\n",
       "      <td>268.067322</td>\n",
       "      <td>109208400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6330</th>\n",
       "      <td>270.940002</td>\n",
       "      <td>271.670013</td>\n",
       "      <td>270.179993</td>\n",
       "      <td>268.523254</td>\n",
       "      <td>59757300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331</th>\n",
       "      <td>270.899994</td>\n",
       "      <td>273.269989</td>\n",
       "      <td>270.190002</td>\n",
       "      <td>268.007874</td>\n",
       "      <td>78709600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6332</th>\n",
       "      <td>267.910004</td>\n",
       "      <td>268.869995</td>\n",
       "      <td>263.359985</td>\n",
       "      <td>261.308441</td>\n",
       "      <td>148785900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6333</th>\n",
       "      <td>264.170013</td>\n",
       "      <td>264.540009</td>\n",
       "      <td>257.829987</td>\n",
       "      <td>255.738739</td>\n",
       "      <td>183534800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6334</th>\n",
       "      <td>262.130005</td>\n",
       "      <td>265.429993</td>\n",
       "      <td>259.410004</td>\n",
       "      <td>262.735504</td>\n",
       "      <td>141956100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6335</th>\n",
       "      <td>266.170013</td>\n",
       "      <td>266.769989</td>\n",
       "      <td>258.839996</td>\n",
       "      <td>258.265930</td>\n",
       "      <td>129941400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6336</th>\n",
       "      <td>260.750000</td>\n",
       "      <td>262.640015</td>\n",
       "      <td>258.579987</td>\n",
       "      <td>257.502777</td>\n",
       "      <td>146452300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6337</th>\n",
       "      <td>261.119995</td>\n",
       "      <td>265.260010</td>\n",
       "      <td>259.839996</td>\n",
       "      <td>260.793060</td>\n",
       "      <td>111601600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6338</th>\n",
       "      <td>262.549988</td>\n",
       "      <td>263.130005</td>\n",
       "      <td>254.669998</td>\n",
       "      <td>255.163956</td>\n",
       "      <td>186286300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6339</th>\n",
       "      <td>258.869995</td>\n",
       "      <td>261.309998</td>\n",
       "      <td>256.839996</td>\n",
       "      <td>258.434387</td>\n",
       "      <td>119956900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6340</th>\n",
       "      <td>256.750000</td>\n",
       "      <td>264.359985</td>\n",
       "      <td>256.600006</td>\n",
       "      <td>261.199402</td>\n",
       "      <td>123715300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6341</th>\n",
       "      <td>265.549988</td>\n",
       "      <td>266.640015</td>\n",
       "      <td>264.320007</td>\n",
       "      <td>263.260773</td>\n",
       "      <td>82652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6342</th>\n",
       "      <td>263.420013</td>\n",
       "      <td>265.109985</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>257.393829</td>\n",
       "      <td>179521200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6343</th>\n",
       "      <td>261.369995</td>\n",
       "      <td>264.839996</td>\n",
       "      <td>259.940002</td>\n",
       "      <td>258.662323</td>\n",
       "      <td>105442900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6344</th>\n",
       "      <td>264.269989</td>\n",
       "      <td>266.040009</td>\n",
       "      <td>262.980011</td>\n",
       "      <td>262.775146</td>\n",
       "      <td>103529000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6345</th>\n",
       "      <td>263.470001</td>\n",
       "      <td>265.640015</td>\n",
       "      <td>263.390015</td>\n",
       "      <td>261.397644</td>\n",
       "      <td>91140200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6346 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open        High         Low   Adj Close     Volume\n",
       "0      43.968700   43.968700   43.750000   27.112253    1003200\n",
       "1      43.968700   44.250000   43.968700   27.305082     480500\n",
       "2      44.218700   44.375000   44.125000   27.362904     201300\n",
       "3      44.406200   44.843700   44.375000   27.652185     529400\n",
       "4      44.968700   45.093700   44.468700   27.767893     531500\n",
       "5      44.968700   45.062500   44.718700   27.748577     492100\n",
       "6      44.968700   45.125000   44.906200   27.748577     596100\n",
       "7      44.812500   44.812500   44.562500   27.555738     122100\n",
       "8      44.656200   44.750000   44.531200   27.594305     379600\n",
       "9      44.781200   45.125000   44.781200   27.729336      19500\n",
       "10     44.875000   44.875000   44.593700   27.517174      42500\n",
       "11     44.468700   44.468700   43.406200   26.822969     374800\n",
       "12     43.468700   43.531200   43.281200   26.803724     210900\n",
       "13     43.937500   43.937500   42.812500   26.784391     378100\n",
       "14     43.406200   43.562500   43.343700   26.880852      34900\n",
       "15     43.687500   43.781200   43.562500   26.977249     513600\n",
       "16     43.843700   43.875000   43.468700   26.958002     373700\n",
       "17     43.718700   44.250000   43.718700   27.305082      26300\n",
       "18     44.218700   44.375000   44.125000   27.362904      44500\n",
       "19     44.437500   44.437500   44.187500   27.401474      66200\n",
       "20     44.562500   44.562500   44.218700   27.324339      66500\n",
       "21     44.312500   44.937500   44.250000   27.729336     182400\n",
       "22     45.000000   45.156200   44.937500   27.845016     280100\n",
       "23     45.187500   45.187500   44.875000   27.690750      89500\n",
       "24     44.937500   45.125000   44.718700   27.613615      40000\n",
       "25     44.843700   45.750000   44.843700   28.230696      50800\n",
       "26     45.656200   45.687500   45.500000   28.134245     169300\n",
       "27     45.593700   45.687500   45.406200   28.192129     194400\n",
       "28     45.718700   45.843700   45.500000   28.114969      70900\n",
       "29     45.187500   45.218700   44.812500   27.825703     643600\n",
       "...          ...         ...         ...         ...        ...\n",
       "6316  275.679993  276.190002  271.290009  268.143005  121907800\n",
       "6317  271.410004  273.170013  266.000000  264.244049  176855100\n",
       "6318  265.799988  269.720001  264.820007  265.606201  139083200\n",
       "6319  267.730011  272.890015  267.609985  268.676025   97307400\n",
       "6320  273.299988  273.390015  271.179993  269.357147   79213200\n",
       "6321  270.420013  273.179993  270.200012  269.258453   87063500\n",
       "6322  273.549988  274.239990  272.420013  270.561401   66901200\n",
       "6323  275.700012  278.869995  275.339996  275.269775  113625300\n",
       "6324  279.200012  279.910004  278.079987  274.924316   71924800\n",
       "6325  279.839996  280.410004  276.029999  273.147583   91968900\n",
       "6326  277.809998  278.019989  274.670013  271.745880  105895100\n",
       "6327  275.880005  276.609985  274.429993  271.449738   83433000\n",
       "6328  274.500000  275.390015  274.140015  271.744141  100343700\n",
       "6329  273.350006  274.399994  268.619995  268.067322  109208400\n",
       "6330  270.940002  271.670013  270.179993  268.523254   59757300\n",
       "6331  270.899994  273.269989  270.190002  268.007874   78709600\n",
       "6332  267.910004  268.869995  263.359985  261.308441  148785900\n",
       "6333  264.170013  264.540009  257.829987  255.738739  183534800\n",
       "6334  262.130005  265.429993  259.410004  262.735504  141956100\n",
       "6335  266.170013  266.769989  258.839996  258.265930  129941400\n",
       "6336  260.750000  262.640015  258.579987  257.502777  146452300\n",
       "6337  261.119995  265.260010  259.839996  260.793060  111601600\n",
       "6338  262.549988  263.130005  254.669998  255.163956  186286300\n",
       "6339  258.869995  261.309998  256.839996  258.434387  119956900\n",
       "6340  256.750000  264.359985  256.600006  261.199402  123715300\n",
       "6341  265.549988  266.640015  264.320007  263.260773   82652600\n",
       "6342  263.420013  265.109985  258.000000  257.393829  179521200\n",
       "6343  261.369995  264.839996  259.940002  258.662323  105442900\n",
       "6344  264.269989  266.040009  262.980011  262.775146  103529000\n",
       "6345  263.470001  265.640015  263.390015  261.397644   91140200\n",
       "\n",
       "[6346 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readTrain():\n",
    "    train = pd.read_csv(\"SPY.csv\")\n",
    "    return train\n",
    "\n",
    "train = readTrain()\n",
    "train = train.drop([\"Date\"], axis=1)\n",
    "train = train.drop([\"Close\"], axis=1)\n",
    "train\n",
    "\n",
    "# TODO: add \"is_music\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train):\n",
    "    train = train.drop([\"Date\"], axis=1)\n",
    "    train_norm = train.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "    return train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastDay=30, futureDay=5):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureDay-pastDay):\n",
    "        X_train.append(np.array(train.iloc[i:i+pastDay]))\n",
    "        Y_train.append(np.array(train.iloc[i+pastDay:i+pastDay+futureDay][\"Adj Close\"]))\n",
    "    return np.array(X_train), np.zeros(np.array(Y_train).shape)\n",
    "    # TODO: return is_music, instead of return futureDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X,Y):\n",
    "    np.random.seed(10)\n",
    "    randomList = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randomList)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(X,Y,rate):\n",
    "    X_train = X[int(X.shape[0]*rate):]\n",
    "    Y_train = Y[int(Y.shape[0]*rate):]\n",
    "    X_val = X[:int(X.shape[0]*rate)]\n",
    "    Y_val = Y[:int(Y.shape[0]*rate)]\n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-83eb3df94bf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# build Data, use last 30 days to predict next 5 days\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# shuffle the data, and random seed is 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-7d0f7259b32f>\u001b[0m in \u001b[0;36mbuildTrain\u001b[0;34m(train, pastDay, futureDay)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpastDay\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpastDay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpastDay\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfutureDay\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Adj Close\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# TODO: return is_music, instead of return futureDay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "# read SPY.csv\n",
    "train = readTrain()\n",
    "\n",
    "# Normalization\n",
    "train_norm = normalize(train)\n",
    "\n",
    "# build Data, use last 30 days to predict next 5 days\n",
    "X_train, Y_train = buildTrain(train_norm, 30, 5)\n",
    "\n",
    "# shuffle the data, and random seed is 10\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "# split training data and validation data\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "print(X_train.shape)\n",
    "# X_trian: (5710, 30, 10)\n",
    "# Y_train: (5710, 5, 1)\n",
    "# X_val: (634, 30, 10)\n",
    "# Y_val: (634, 5, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One to one model  \n",
    "input shape:  \n",
    "(5710, 1, 10)  \n",
    "output shape:  \n",
    "(5710, 1, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildOneToOneModel(shape):\n",
    "    #print(shape)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(5, input_length=shape[1], input_dim=shape[2],return_sequences=True))\n",
    "    # output shape: (1, 1)\n",
    "    model.add(TimeDistributed(Dense(1)))    # or use model.add(Dense(1))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5710, 1, 6)\n",
      "(5710, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab/anaconda3/envs/keras/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/ailab/anaconda3/envs/keras/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(5, return_sequences=True, input_shape=(1, 6))`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 5)              240       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 1)              6         \n",
      "=================================================================\n",
      "Total params: 246\n",
      "Trainable params: 246\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5710 samples, validate on 634 samples\n",
      "Epoch 1/1000\n",
      "5710/5710 [==============================] - 1s 131us/step - loss: 0.0397 - val_loss: 0.0796\n",
      "Epoch 2/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 0.0272 - val_loss: 0.0507\n",
      "Epoch 3/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 0.0166 - val_loss: 0.0321\n",
      "Epoch 4/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 0.0082 - val_loss: 0.0171\n",
      "Epoch 5/1000\n",
      "5710/5710 [==============================] - 0s 17us/step - loss: 0.0030 - val_loss: 0.0088\n",
      "Epoch 6/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 8.5319e-04 - val_loss: 0.0050\n",
      "Epoch 7/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 3.6114e-04 - val_loss: 0.0037\n",
      "Epoch 8/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 2.8723e-04 - val_loss: 0.0031\n",
      "Epoch 9/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 2.6728e-04 - val_loss: 0.0027\n",
      "Epoch 10/1000\n",
      "5710/5710 [==============================] - 0s 17us/step - loss: 2.5444e-04 - val_loss: 0.0025\n",
      "Epoch 11/1000\n",
      "5710/5710 [==============================] - 0s 17us/step - loss: 2.4492e-04 - val_loss: 0.0023\n",
      "Epoch 12/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 2.3791e-04 - val_loss: 0.0021\n",
      "Epoch 13/1000\n",
      "5710/5710 [==============================] - 0s 17us/step - loss: 2.3292e-04 - val_loss: 0.0019\n",
      "Epoch 14/1000\n",
      "5710/5710 [==============================] - 0s 17us/step - loss: 2.2801e-04 - val_loss: 0.0017\n",
      "Epoch 15/1000\n",
      "5710/5710 [==============================] - 0s 17us/step - loss: 2.2390e-04 - val_loss: 0.0016\n",
      "Epoch 16/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 2.2094e-04 - val_loss: 0.0015\n",
      "Epoch 17/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 2.1779e-04 - val_loss: 0.0015\n",
      "Epoch 18/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.1473e-04 - val_loss: 0.0014\n",
      "Epoch 19/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 2.1216e-04 - val_loss: 0.0014\n",
      "Epoch 20/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 2.0935e-04 - val_loss: 0.0013\n",
      "Epoch 21/1000\n",
      "5710/5710 [==============================] - 0s 17us/step - loss: 2.0666e-04 - val_loss: 0.0013\n",
      "Epoch 22/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 2.0387e-04 - val_loss: 0.0013\n",
      "Epoch 23/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 2.0071e-04 - val_loss: 0.0012\n",
      "Epoch 24/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 1.9790e-04 - val_loss: 0.0012\n",
      "Epoch 25/1000\n",
      "5710/5710 [==============================] - 0s 18us/step - loss: 1.9488e-04 - val_loss: 0.0012\n",
      "Epoch 26/1000\n",
      "5710/5710 [==============================] - 0s 18us/step - loss: 1.9163e-04 - val_loss: 0.0011\n",
      "Epoch 27/1000\n",
      "5710/5710 [==============================] - 0s 17us/step - loss: 1.8857e-04 - val_loss: 0.0012\n",
      "Epoch 28/1000\n",
      "5710/5710 [==============================] - 0s 17us/step - loss: 1.8536e-04 - val_loss: 0.0011\n",
      "Epoch 29/1000\n",
      "5710/5710 [==============================] - 0s 17us/step - loss: 1.8233e-04 - val_loss: 0.0011\n",
      "Epoch 30/1000\n",
      "5710/5710 [==============================] - 0s 17us/step - loss: 1.7938e-04 - val_loss: 0.0011\n",
      "Epoch 31/1000\n",
      "5710/5710 [==============================] - 0s 19us/step - loss: 1.7613e-04 - val_loss: 0.0011\n",
      "Epoch 32/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 1.7312e-04 - val_loss: 0.0011\n",
      "Epoch 33/1000\n",
      "5710/5710 [==============================] - 0s 17us/step - loss: 1.6958e-04 - val_loss: 0.0011\n",
      "Epoch 34/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 1.6653e-04 - val_loss: 0.0011\n",
      "Epoch 35/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 1.6385e-04 - val_loss: 0.0010\n",
      "Epoch 36/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 1.6031e-04 - val_loss: 0.0010\n",
      "Epoch 37/1000\n",
      "5710/5710 [==============================] - 0s 17us/step - loss: 1.5699e-04 - val_loss: 0.0010\n",
      "Epoch 38/1000\n",
      "5710/5710 [==============================] - 0s 17us/step - loss: 1.5383e-04 - val_loss: 9.9852e-04\n",
      "Epoch 39/1000\n",
      "5710/5710 [==============================] - 0s 17us/step - loss: 1.5082e-04 - val_loss: 9.3670e-04\n",
      "Epoch 40/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 1.4790e-04 - val_loss: 0.0010\n",
      "Epoch 41/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.4487e-04 - val_loss: 9.3396e-04\n",
      "Epoch 42/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 1.4200e-04 - val_loss: 9.2686e-04\n",
      "Epoch 43/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 1.3954e-04 - val_loss: 8.7577e-04\n",
      "Epoch 44/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.3716e-04 - val_loss: 9.0237e-04\n",
      "Epoch 45/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.3432e-04 - val_loss: 9.3813e-04\n",
      "Epoch 46/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 1.3187e-04 - val_loss: 8.5475e-04\n",
      "Epoch 47/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.2893e-04 - val_loss: 8.6544e-04\n",
      "Epoch 48/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.2630e-04 - val_loss: 8.6147e-04\n",
      "Epoch 49/1000\n",
      "5710/5710 [==============================] - 0s 17us/step - loss: 1.2448e-04 - val_loss: 7.9620e-04\n",
      "Epoch 50/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 1.2230e-04 - val_loss: 8.6323e-04\n",
      "Epoch 51/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 1.1958e-04 - val_loss: 8.4924e-04\n",
      "Epoch 52/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.1765e-04 - val_loss: 8.1580e-04\n",
      "Epoch 53/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 1.1566e-04 - val_loss: 8.0117e-04\n",
      "Epoch 54/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 1.1350e-04 - val_loss: 6.9395e-04\n",
      "Epoch 55/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.1195e-04 - val_loss: 7.5178e-04\n",
      "Epoch 56/1000\n",
      "5710/5710 [==============================] - 0s 18us/step - loss: 1.1008e-04 - val_loss: 7.6899e-04\n",
      "Epoch 57/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 1.0814e-04 - val_loss: 6.7382e-04\n",
      "Epoch 58/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 1.0604e-04 - val_loss: 7.0170e-04\n",
      "Epoch 59/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 1.0452e-04 - val_loss: 7.1698e-04\n",
      "Epoch 60/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.0241e-04 - val_loss: 6.6774e-04\n",
      "Epoch 61/1000\n",
      "5710/5710 [==============================] - 0s 17us/step - loss: 1.0055e-04 - val_loss: 6.9708e-04\n",
      "Epoch 62/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 9.9242e-05 - val_loss: 7.1890e-04\n",
      "Epoch 63/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 9.7530e-05 - val_loss: 6.4137e-04\n",
      "Epoch 64/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 9.5442e-05 - val_loss: 6.2692e-04\n",
      "Epoch 65/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 9.3684e-05 - val_loss: 6.3345e-04\n",
      "Epoch 66/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 9.2105e-05 - val_loss: 5.7619e-04\n",
      "Epoch 67/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 9.0439e-05 - val_loss: 6.0991e-04\n",
      "Epoch 68/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 8.8721e-05 - val_loss: 5.2176e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 8.7350e-05 - val_loss: 5.6908e-04\n",
      "Epoch 70/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 8.5389e-05 - val_loss: 5.5755e-04\n",
      "Epoch 71/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 8.3709e-05 - val_loss: 4.7819e-04\n",
      "Epoch 72/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 8.2500e-05 - val_loss: 5.0618e-04\n",
      "Epoch 73/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 8.0312e-05 - val_loss: 4.7849e-04\n",
      "Epoch 74/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 7.8201e-05 - val_loss: 4.9652e-04\n",
      "Epoch 75/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 7.6950e-05 - val_loss: 4.7023e-04\n",
      "Epoch 76/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 7.5158e-05 - val_loss: 4.9335e-04\n",
      "Epoch 77/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 7.3575e-05 - val_loss: 4.6704e-04\n",
      "Epoch 78/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 7.1791e-05 - val_loss: 4.3827e-04\n",
      "Epoch 79/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 7.0005e-05 - val_loss: 4.2981e-04\n",
      "Epoch 80/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 6.8476e-05 - val_loss: 4.2620e-04\n",
      "Epoch 81/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 6.6657e-05 - val_loss: 3.9016e-04\n",
      "Epoch 82/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 6.5183e-05 - val_loss: 3.9255e-04\n",
      "Epoch 83/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 6.3685e-05 - val_loss: 3.8298e-04\n",
      "Epoch 84/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 6.2447e-05 - val_loss: 3.5558e-04\n",
      "Epoch 85/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 6.0944e-05 - val_loss: 3.6802e-04\n",
      "Epoch 86/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 5.9432e-05 - val_loss: 3.6622e-04\n",
      "Epoch 87/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 5.7975e-05 - val_loss: 3.3114e-04\n",
      "Epoch 88/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 5.6004e-05 - val_loss: 3.5930e-04\n",
      "Epoch 89/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 5.4776e-05 - val_loss: 2.8943e-04\n",
      "Epoch 90/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.3886e-05 - val_loss: 2.4451e-04\n",
      "Epoch 91/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 5.2418e-05 - val_loss: 2.6253e-04\n",
      "Epoch 92/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 5.1588e-05 - val_loss: 2.8011e-04\n",
      "Epoch 93/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 4.9497e-05 - val_loss: 2.3486e-04\n",
      "Epoch 94/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.8992e-05 - val_loss: 2.2879e-04\n",
      "Epoch 95/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.7679e-05 - val_loss: 2.7298e-04\n",
      "Epoch 96/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.6522e-05 - val_loss: 2.4498e-04\n",
      "Epoch 97/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 4.5755e-05 - val_loss: 2.3909e-04\n",
      "Epoch 98/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.4273e-05 - val_loss: 2.1666e-04\n",
      "Epoch 99/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.3200e-05 - val_loss: 2.0991e-04\n",
      "Epoch 100/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 4.2327e-05 - val_loss: 1.7134e-04\n",
      "Epoch 101/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.1426e-05 - val_loss: 2.0224e-04\n",
      "Epoch 102/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 4.0397e-05 - val_loss: 1.6275e-04\n",
      "Epoch 103/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 4.0656e-05 - val_loss: 1.5099e-04\n",
      "Epoch 104/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 3.8805e-05 - val_loss: 1.4357e-04\n",
      "Epoch 105/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 3.8116e-05 - val_loss: 1.5938e-04\n",
      "Epoch 106/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 3.7734e-05 - val_loss: 1.4825e-04\n",
      "Epoch 107/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.6828e-05 - val_loss: 1.6517e-04\n",
      "Epoch 108/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 3.6980e-05 - val_loss: 1.3292e-04\n",
      "Epoch 109/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 3.5778e-05 - val_loss: 1.0882e-04\n",
      "Epoch 110/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.5221e-05 - val_loss: 1.1571e-04\n",
      "Epoch 111/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 3.4479e-05 - val_loss: 1.1972e-04\n",
      "Epoch 112/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 3.4070e-05 - val_loss: 1.0977e-04\n",
      "Epoch 113/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 3.3455e-05 - val_loss: 9.6078e-05\n",
      "Epoch 114/1000\n",
      " 128/5710 [..............................] - ETA: 0s - loss: 2.9351e-05"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-37011a2a4503>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildOneToOneModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_norm = normalize(train)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 1, 1)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,np.newaxis]\n",
    "Y_val = Y_val[:,np.newaxis]\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "model = buildOneToOneModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many to one model  \n",
    "input shape:  \n",
    "(5710, 1, 10)  \n",
    "output shape:  \n",
    "(5710, 1, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildManyToOneModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(5, input_length=shape[1], input_dim=shape[2]))\n",
    "    # output shape: (1, 1)\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=['binary_accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5684, 30, 6)\n",
      "(5684, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab/anaconda3/envs/keras/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/ailab/anaconda3/envs/keras/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(5, input_shape=(30, 6))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 5)                 240       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 246\n",
      "Trainable params: 246\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5684 samples, validate on 631 samples\n",
      "Epoch 1/10\n",
      "5684/5684 [==============================] - 2s 296us/step - loss: 0.0017 - binary_accuracy: 1.0000 - val_loss: 1.5745e-04 - val_binary_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 3.1508e-04 - binary_accuracy: 1.0000 - val_loss: 2.5965e-04 - val_binary_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 7.7430e-05 - binary_accuracy: 1.0000 - val_loss: 9.5375e-05 - val_binary_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 4.6760e-05 - binary_accuracy: 1.0000 - val_loss: 5.9411e-05 - val_binary_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "5684/5684 [==============================] - 1s 102us/step - loss: 3.2625e-05 - binary_accuracy: 1.0000 - val_loss: 1.1374e-05 - val_binary_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "5684/5684 [==============================] - 1s 102us/step - loss: 2.3114e-05 - binary_accuracy: 1.0000 - val_loss: 8.5119e-06 - val_binary_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 1.7196e-05 - binary_accuracy: 1.0000 - val_loss: 1.9843e-07 - val_binary_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "5684/5684 [==============================] - 1s 102us/step - loss: 1.2947e-05 - binary_accuracy: 1.0000 - val_loss: 4.0603e-07 - val_binary_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 1.0305e-05 - binary_accuracy: 1.0000 - val_loss: 3.2745e-06 - val_binary_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 8.5593e-06 - binary_accuracy: 1.0000 - val_loss: 3.1509e-06 - val_binary_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f424552ce80>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_norm = normalize(train)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 30, 1)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "# because no return sequence, Y_train and Y_val shape must be 2 dimension\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "model = buildManyToOneModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631/631 [==============================] - 0s 82us/step\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0026771757174607973, 0.0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
