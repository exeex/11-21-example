{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@daniel820710/%E5%88%A9%E7%94%A8keras%E5%BB%BA%E6%A7%8Blstm%E6%A8%A1%E5%9E%8B-%E4%BB%A5stock-prediction-%E7%82%BA%E4%BE%8B-1-67456e0a0b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab/anaconda3/envs/keras/lib/python3.6/site-packages/matplotlib/font_manager.py:278: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1993-01-29</td>\n",
       "      <td>43.968700</td>\n",
       "      <td>43.968700</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>43.937500</td>\n",
       "      <td>27.112253</td>\n",
       "      <td>1003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1993-02-01</td>\n",
       "      <td>43.968700</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>43.968700</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>27.305082</td>\n",
       "      <td>480500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1993-02-02</td>\n",
       "      <td>44.218700</td>\n",
       "      <td>44.375000</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>44.343700</td>\n",
       "      <td>27.362904</td>\n",
       "      <td>201300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1993-02-03</td>\n",
       "      <td>44.406200</td>\n",
       "      <td>44.843700</td>\n",
       "      <td>44.375000</td>\n",
       "      <td>44.812500</td>\n",
       "      <td>27.652185</td>\n",
       "      <td>529400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1993-02-04</td>\n",
       "      <td>44.968700</td>\n",
       "      <td>45.093700</td>\n",
       "      <td>44.468700</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>27.767893</td>\n",
       "      <td>531500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1993-02-05</td>\n",
       "      <td>44.968700</td>\n",
       "      <td>45.062500</td>\n",
       "      <td>44.718700</td>\n",
       "      <td>44.968700</td>\n",
       "      <td>27.748577</td>\n",
       "      <td>492100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1993-02-08</td>\n",
       "      <td>44.968700</td>\n",
       "      <td>45.125000</td>\n",
       "      <td>44.906200</td>\n",
       "      <td>44.968700</td>\n",
       "      <td>27.748577</td>\n",
       "      <td>596100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1993-02-09</td>\n",
       "      <td>44.812500</td>\n",
       "      <td>44.812500</td>\n",
       "      <td>44.562500</td>\n",
       "      <td>44.656200</td>\n",
       "      <td>27.555738</td>\n",
       "      <td>122100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1993-02-10</td>\n",
       "      <td>44.656200</td>\n",
       "      <td>44.750000</td>\n",
       "      <td>44.531200</td>\n",
       "      <td>44.718700</td>\n",
       "      <td>27.594305</td>\n",
       "      <td>379600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1993-02-11</td>\n",
       "      <td>44.781200</td>\n",
       "      <td>45.125000</td>\n",
       "      <td>44.781200</td>\n",
       "      <td>44.937500</td>\n",
       "      <td>27.729336</td>\n",
       "      <td>19500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1993-02-12</td>\n",
       "      <td>44.875000</td>\n",
       "      <td>44.875000</td>\n",
       "      <td>44.593700</td>\n",
       "      <td>44.593700</td>\n",
       "      <td>27.517174</td>\n",
       "      <td>42500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1993-02-16</td>\n",
       "      <td>44.468700</td>\n",
       "      <td>44.468700</td>\n",
       "      <td>43.406200</td>\n",
       "      <td>43.468700</td>\n",
       "      <td>26.822969</td>\n",
       "      <td>374800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1993-02-17</td>\n",
       "      <td>43.468700</td>\n",
       "      <td>43.531200</td>\n",
       "      <td>43.281200</td>\n",
       "      <td>43.437500</td>\n",
       "      <td>26.803724</td>\n",
       "      <td>210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1993-02-18</td>\n",
       "      <td>43.937500</td>\n",
       "      <td>43.937500</td>\n",
       "      <td>42.812500</td>\n",
       "      <td>43.406200</td>\n",
       "      <td>26.784391</td>\n",
       "      <td>378100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1993-02-19</td>\n",
       "      <td>43.406200</td>\n",
       "      <td>43.562500</td>\n",
       "      <td>43.343700</td>\n",
       "      <td>43.562500</td>\n",
       "      <td>26.880852</td>\n",
       "      <td>34900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1993-02-22</td>\n",
       "      <td>43.687500</td>\n",
       "      <td>43.781200</td>\n",
       "      <td>43.562500</td>\n",
       "      <td>43.718700</td>\n",
       "      <td>26.977249</td>\n",
       "      <td>513600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1993-02-23</td>\n",
       "      <td>43.843700</td>\n",
       "      <td>43.875000</td>\n",
       "      <td>43.468700</td>\n",
       "      <td>43.687500</td>\n",
       "      <td>26.958002</td>\n",
       "      <td>373700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1993-02-24</td>\n",
       "      <td>43.718700</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>43.718700</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>27.305082</td>\n",
       "      <td>26300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1993-02-25</td>\n",
       "      <td>44.218700</td>\n",
       "      <td>44.375000</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>44.343700</td>\n",
       "      <td>27.362904</td>\n",
       "      <td>44500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1993-02-26</td>\n",
       "      <td>44.437500</td>\n",
       "      <td>44.437500</td>\n",
       "      <td>44.187500</td>\n",
       "      <td>44.406200</td>\n",
       "      <td>27.401474</td>\n",
       "      <td>66200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1993-03-01</td>\n",
       "      <td>44.562500</td>\n",
       "      <td>44.562500</td>\n",
       "      <td>44.218700</td>\n",
       "      <td>44.281200</td>\n",
       "      <td>27.324339</td>\n",
       "      <td>66500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1993-03-02</td>\n",
       "      <td>44.312500</td>\n",
       "      <td>44.937500</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>44.937500</td>\n",
       "      <td>27.729336</td>\n",
       "      <td>182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1993-03-03</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.156200</td>\n",
       "      <td>44.937500</td>\n",
       "      <td>45.125000</td>\n",
       "      <td>27.845016</td>\n",
       "      <td>280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1993-03-04</td>\n",
       "      <td>45.187500</td>\n",
       "      <td>45.187500</td>\n",
       "      <td>44.875000</td>\n",
       "      <td>44.875000</td>\n",
       "      <td>27.690750</td>\n",
       "      <td>89500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1993-03-05</td>\n",
       "      <td>44.937500</td>\n",
       "      <td>45.125000</td>\n",
       "      <td>44.718700</td>\n",
       "      <td>44.750000</td>\n",
       "      <td>27.613615</td>\n",
       "      <td>40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1993-03-08</td>\n",
       "      <td>44.843700</td>\n",
       "      <td>45.750000</td>\n",
       "      <td>44.843700</td>\n",
       "      <td>45.750000</td>\n",
       "      <td>28.230696</td>\n",
       "      <td>50800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1993-03-09</td>\n",
       "      <td>45.656200</td>\n",
       "      <td>45.687500</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>45.593700</td>\n",
       "      <td>28.134245</td>\n",
       "      <td>169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1993-03-10</td>\n",
       "      <td>45.593700</td>\n",
       "      <td>45.687500</td>\n",
       "      <td>45.406200</td>\n",
       "      <td>45.687500</td>\n",
       "      <td>28.192129</td>\n",
       "      <td>194400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1993-03-11</td>\n",
       "      <td>45.718700</td>\n",
       "      <td>45.843700</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>45.562500</td>\n",
       "      <td>28.114969</td>\n",
       "      <td>70900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1993-03-12</td>\n",
       "      <td>45.187500</td>\n",
       "      <td>45.218700</td>\n",
       "      <td>44.812500</td>\n",
       "      <td>45.093700</td>\n",
       "      <td>27.825703</td>\n",
       "      <td>643600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6316</th>\n",
       "      <td>2018-02-28</td>\n",
       "      <td>275.679993</td>\n",
       "      <td>276.190002</td>\n",
       "      <td>271.290009</td>\n",
       "      <td>271.649994</td>\n",
       "      <td>268.143005</td>\n",
       "      <td>121907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6317</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>271.410004</td>\n",
       "      <td>273.170013</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>267.700012</td>\n",
       "      <td>264.244049</td>\n",
       "      <td>176855100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6318</th>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>265.799988</td>\n",
       "      <td>269.720001</td>\n",
       "      <td>264.820007</td>\n",
       "      <td>269.079987</td>\n",
       "      <td>265.606201</td>\n",
       "      <td>139083200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6319</th>\n",
       "      <td>2018-03-05</td>\n",
       "      <td>267.730011</td>\n",
       "      <td>272.890015</td>\n",
       "      <td>267.609985</td>\n",
       "      <td>272.190002</td>\n",
       "      <td>268.676025</td>\n",
       "      <td>97307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6320</th>\n",
       "      <td>2018-03-06</td>\n",
       "      <td>273.299988</td>\n",
       "      <td>273.390015</td>\n",
       "      <td>271.179993</td>\n",
       "      <td>272.880005</td>\n",
       "      <td>269.357147</td>\n",
       "      <td>79213200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6321</th>\n",
       "      <td>2018-03-07</td>\n",
       "      <td>270.420013</td>\n",
       "      <td>273.179993</td>\n",
       "      <td>270.200012</td>\n",
       "      <td>272.779999</td>\n",
       "      <td>269.258453</td>\n",
       "      <td>87063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6322</th>\n",
       "      <td>2018-03-08</td>\n",
       "      <td>273.549988</td>\n",
       "      <td>274.239990</td>\n",
       "      <td>272.420013</td>\n",
       "      <td>274.100006</td>\n",
       "      <td>270.561401</td>\n",
       "      <td>66901200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6323</th>\n",
       "      <td>2018-03-09</td>\n",
       "      <td>275.700012</td>\n",
       "      <td>278.869995</td>\n",
       "      <td>275.339996</td>\n",
       "      <td>278.869995</td>\n",
       "      <td>275.269775</td>\n",
       "      <td>113625300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6324</th>\n",
       "      <td>2018-03-12</td>\n",
       "      <td>279.200012</td>\n",
       "      <td>279.910004</td>\n",
       "      <td>278.079987</td>\n",
       "      <td>278.519989</td>\n",
       "      <td>274.924316</td>\n",
       "      <td>71924800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6325</th>\n",
       "      <td>2018-03-13</td>\n",
       "      <td>279.839996</td>\n",
       "      <td>280.410004</td>\n",
       "      <td>276.029999</td>\n",
       "      <td>276.720001</td>\n",
       "      <td>273.147583</td>\n",
       "      <td>91968900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6326</th>\n",
       "      <td>2018-03-14</td>\n",
       "      <td>277.809998</td>\n",
       "      <td>278.019989</td>\n",
       "      <td>274.670013</td>\n",
       "      <td>275.299988</td>\n",
       "      <td>271.745880</td>\n",
       "      <td>105895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6327</th>\n",
       "      <td>2018-03-15</td>\n",
       "      <td>275.880005</td>\n",
       "      <td>276.609985</td>\n",
       "      <td>274.429993</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>271.449738</td>\n",
       "      <td>83433000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6328</th>\n",
       "      <td>2018-03-16</td>\n",
       "      <td>274.500000</td>\n",
       "      <td>275.390015</td>\n",
       "      <td>274.140015</td>\n",
       "      <td>274.200012</td>\n",
       "      <td>271.744141</td>\n",
       "      <td>100343700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6329</th>\n",
       "      <td>2018-03-19</td>\n",
       "      <td>273.350006</td>\n",
       "      <td>274.399994</td>\n",
       "      <td>268.619995</td>\n",
       "      <td>270.489990</td>\n",
       "      <td>268.067322</td>\n",
       "      <td>109208400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6330</th>\n",
       "      <td>2018-03-20</td>\n",
       "      <td>270.940002</td>\n",
       "      <td>271.670013</td>\n",
       "      <td>270.179993</td>\n",
       "      <td>270.950012</td>\n",
       "      <td>268.523254</td>\n",
       "      <td>59757300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331</th>\n",
       "      <td>2018-03-21</td>\n",
       "      <td>270.899994</td>\n",
       "      <td>273.269989</td>\n",
       "      <td>270.190002</td>\n",
       "      <td>270.429993</td>\n",
       "      <td>268.007874</td>\n",
       "      <td>78709600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6332</th>\n",
       "      <td>2018-03-22</td>\n",
       "      <td>267.910004</td>\n",
       "      <td>268.869995</td>\n",
       "      <td>263.359985</td>\n",
       "      <td>263.670013</td>\n",
       "      <td>261.308441</td>\n",
       "      <td>148785900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6333</th>\n",
       "      <td>2018-03-23</td>\n",
       "      <td>264.170013</td>\n",
       "      <td>264.540009</td>\n",
       "      <td>257.829987</td>\n",
       "      <td>258.049988</td>\n",
       "      <td>255.738739</td>\n",
       "      <td>183534800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6334</th>\n",
       "      <td>2018-03-26</td>\n",
       "      <td>262.130005</td>\n",
       "      <td>265.429993</td>\n",
       "      <td>259.410004</td>\n",
       "      <td>265.109985</td>\n",
       "      <td>262.735504</td>\n",
       "      <td>141956100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6335</th>\n",
       "      <td>2018-03-27</td>\n",
       "      <td>266.170013</td>\n",
       "      <td>266.769989</td>\n",
       "      <td>258.839996</td>\n",
       "      <td>260.600006</td>\n",
       "      <td>258.265930</td>\n",
       "      <td>129941400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6336</th>\n",
       "      <td>2018-03-28</td>\n",
       "      <td>260.750000</td>\n",
       "      <td>262.640015</td>\n",
       "      <td>258.579987</td>\n",
       "      <td>259.829987</td>\n",
       "      <td>257.502777</td>\n",
       "      <td>146452300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6337</th>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>261.119995</td>\n",
       "      <td>265.260010</td>\n",
       "      <td>259.839996</td>\n",
       "      <td>263.149994</td>\n",
       "      <td>260.793060</td>\n",
       "      <td>111601600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6338</th>\n",
       "      <td>2018-04-02</td>\n",
       "      <td>262.549988</td>\n",
       "      <td>263.130005</td>\n",
       "      <td>254.669998</td>\n",
       "      <td>257.470001</td>\n",
       "      <td>255.163956</td>\n",
       "      <td>186286300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6339</th>\n",
       "      <td>2018-04-03</td>\n",
       "      <td>258.869995</td>\n",
       "      <td>261.309998</td>\n",
       "      <td>256.839996</td>\n",
       "      <td>260.769989</td>\n",
       "      <td>258.434387</td>\n",
       "      <td>119956900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6340</th>\n",
       "      <td>2018-04-04</td>\n",
       "      <td>256.750000</td>\n",
       "      <td>264.359985</td>\n",
       "      <td>256.600006</td>\n",
       "      <td>263.559998</td>\n",
       "      <td>261.199402</td>\n",
       "      <td>123715300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6341</th>\n",
       "      <td>2018-04-05</td>\n",
       "      <td>265.549988</td>\n",
       "      <td>266.640015</td>\n",
       "      <td>264.320007</td>\n",
       "      <td>265.640015</td>\n",
       "      <td>263.260773</td>\n",
       "      <td>82652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6342</th>\n",
       "      <td>2018-04-06</td>\n",
       "      <td>263.420013</td>\n",
       "      <td>265.109985</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>259.720001</td>\n",
       "      <td>257.393829</td>\n",
       "      <td>179521200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6343</th>\n",
       "      <td>2018-04-09</td>\n",
       "      <td>261.369995</td>\n",
       "      <td>264.839996</td>\n",
       "      <td>259.940002</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>258.662323</td>\n",
       "      <td>105442900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6344</th>\n",
       "      <td>2018-04-10</td>\n",
       "      <td>264.269989</td>\n",
       "      <td>266.040009</td>\n",
       "      <td>262.980011</td>\n",
       "      <td>265.149994</td>\n",
       "      <td>262.775146</td>\n",
       "      <td>103529000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6345</th>\n",
       "      <td>2018-04-11</td>\n",
       "      <td>263.470001</td>\n",
       "      <td>265.640015</td>\n",
       "      <td>263.390015</td>\n",
       "      <td>263.760010</td>\n",
       "      <td>261.397644</td>\n",
       "      <td>91140200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6346 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "0     1993-01-29   43.968700   43.968700   43.750000   43.937500   27.112253   \n",
       "1     1993-02-01   43.968700   44.250000   43.968700   44.250000   27.305082   \n",
       "2     1993-02-02   44.218700   44.375000   44.125000   44.343700   27.362904   \n",
       "3     1993-02-03   44.406200   44.843700   44.375000   44.812500   27.652185   \n",
       "4     1993-02-04   44.968700   45.093700   44.468700   45.000000   27.767893   \n",
       "5     1993-02-05   44.968700   45.062500   44.718700   44.968700   27.748577   \n",
       "6     1993-02-08   44.968700   45.125000   44.906200   44.968700   27.748577   \n",
       "7     1993-02-09   44.812500   44.812500   44.562500   44.656200   27.555738   \n",
       "8     1993-02-10   44.656200   44.750000   44.531200   44.718700   27.594305   \n",
       "9     1993-02-11   44.781200   45.125000   44.781200   44.937500   27.729336   \n",
       "10    1993-02-12   44.875000   44.875000   44.593700   44.593700   27.517174   \n",
       "11    1993-02-16   44.468700   44.468700   43.406200   43.468700   26.822969   \n",
       "12    1993-02-17   43.468700   43.531200   43.281200   43.437500   26.803724   \n",
       "13    1993-02-18   43.937500   43.937500   42.812500   43.406200   26.784391   \n",
       "14    1993-02-19   43.406200   43.562500   43.343700   43.562500   26.880852   \n",
       "15    1993-02-22   43.687500   43.781200   43.562500   43.718700   26.977249   \n",
       "16    1993-02-23   43.843700   43.875000   43.468700   43.687500   26.958002   \n",
       "17    1993-02-24   43.718700   44.250000   43.718700   44.250000   27.305082   \n",
       "18    1993-02-25   44.218700   44.375000   44.125000   44.343700   27.362904   \n",
       "19    1993-02-26   44.437500   44.437500   44.187500   44.406200   27.401474   \n",
       "20    1993-03-01   44.562500   44.562500   44.218700   44.281200   27.324339   \n",
       "21    1993-03-02   44.312500   44.937500   44.250000   44.937500   27.729336   \n",
       "22    1993-03-03   45.000000   45.156200   44.937500   45.125000   27.845016   \n",
       "23    1993-03-04   45.187500   45.187500   44.875000   44.875000   27.690750   \n",
       "24    1993-03-05   44.937500   45.125000   44.718700   44.750000   27.613615   \n",
       "25    1993-03-08   44.843700   45.750000   44.843700   45.750000   28.230696   \n",
       "26    1993-03-09   45.656200   45.687500   45.500000   45.593700   28.134245   \n",
       "27    1993-03-10   45.593700   45.687500   45.406200   45.687500   28.192129   \n",
       "28    1993-03-11   45.718700   45.843700   45.500000   45.562500   28.114969   \n",
       "29    1993-03-12   45.187500   45.218700   44.812500   45.093700   27.825703   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "6316  2018-02-28  275.679993  276.190002  271.290009  271.649994  268.143005   \n",
       "6317  2018-03-01  271.410004  273.170013  266.000000  267.700012  264.244049   \n",
       "6318  2018-03-02  265.799988  269.720001  264.820007  269.079987  265.606201   \n",
       "6319  2018-03-05  267.730011  272.890015  267.609985  272.190002  268.676025   \n",
       "6320  2018-03-06  273.299988  273.390015  271.179993  272.880005  269.357147   \n",
       "6321  2018-03-07  270.420013  273.179993  270.200012  272.779999  269.258453   \n",
       "6322  2018-03-08  273.549988  274.239990  272.420013  274.100006  270.561401   \n",
       "6323  2018-03-09  275.700012  278.869995  275.339996  278.869995  275.269775   \n",
       "6324  2018-03-12  279.200012  279.910004  278.079987  278.519989  274.924316   \n",
       "6325  2018-03-13  279.839996  280.410004  276.029999  276.720001  273.147583   \n",
       "6326  2018-03-14  277.809998  278.019989  274.670013  275.299988  271.745880   \n",
       "6327  2018-03-15  275.880005  276.609985  274.429993  275.000000  271.449738   \n",
       "6328  2018-03-16  274.500000  275.390015  274.140015  274.200012  271.744141   \n",
       "6329  2018-03-19  273.350006  274.399994  268.619995  270.489990  268.067322   \n",
       "6330  2018-03-20  270.940002  271.670013  270.179993  270.950012  268.523254   \n",
       "6331  2018-03-21  270.899994  273.269989  270.190002  270.429993  268.007874   \n",
       "6332  2018-03-22  267.910004  268.869995  263.359985  263.670013  261.308441   \n",
       "6333  2018-03-23  264.170013  264.540009  257.829987  258.049988  255.738739   \n",
       "6334  2018-03-26  262.130005  265.429993  259.410004  265.109985  262.735504   \n",
       "6335  2018-03-27  266.170013  266.769989  258.839996  260.600006  258.265930   \n",
       "6336  2018-03-28  260.750000  262.640015  258.579987  259.829987  257.502777   \n",
       "6337  2018-03-29  261.119995  265.260010  259.839996  263.149994  260.793060   \n",
       "6338  2018-04-02  262.549988  263.130005  254.669998  257.470001  255.163956   \n",
       "6339  2018-04-03  258.869995  261.309998  256.839996  260.769989  258.434387   \n",
       "6340  2018-04-04  256.750000  264.359985  256.600006  263.559998  261.199402   \n",
       "6341  2018-04-05  265.549988  266.640015  264.320007  265.640015  263.260773   \n",
       "6342  2018-04-06  263.420013  265.109985  258.000000  259.720001  257.393829   \n",
       "6343  2018-04-09  261.369995  264.839996  259.940002  261.000000  258.662323   \n",
       "6344  2018-04-10  264.269989  266.040009  262.980011  265.149994  262.775146   \n",
       "6345  2018-04-11  263.470001  265.640015  263.390015  263.760010  261.397644   \n",
       "\n",
       "         Volume  \n",
       "0       1003200  \n",
       "1        480500  \n",
       "2        201300  \n",
       "3        529400  \n",
       "4        531500  \n",
       "5        492100  \n",
       "6        596100  \n",
       "7        122100  \n",
       "8        379600  \n",
       "9         19500  \n",
       "10        42500  \n",
       "11       374800  \n",
       "12       210900  \n",
       "13       378100  \n",
       "14        34900  \n",
       "15       513600  \n",
       "16       373700  \n",
       "17        26300  \n",
       "18        44500  \n",
       "19        66200  \n",
       "20        66500  \n",
       "21       182400  \n",
       "22       280100  \n",
       "23        89500  \n",
       "24        40000  \n",
       "25        50800  \n",
       "26       169300  \n",
       "27       194400  \n",
       "28        70900  \n",
       "29       643600  \n",
       "...         ...  \n",
       "6316  121907800  \n",
       "6317  176855100  \n",
       "6318  139083200  \n",
       "6319   97307400  \n",
       "6320   79213200  \n",
       "6321   87063500  \n",
       "6322   66901200  \n",
       "6323  113625300  \n",
       "6324   71924800  \n",
       "6325   91968900  \n",
       "6326  105895100  \n",
       "6327   83433000  \n",
       "6328  100343700  \n",
       "6329  109208400  \n",
       "6330   59757300  \n",
       "6331   78709600  \n",
       "6332  148785900  \n",
       "6333  183534800  \n",
       "6334  141956100  \n",
       "6335  129941400  \n",
       "6336  146452300  \n",
       "6337  111601600  \n",
       "6338  186286300  \n",
       "6339  119956900  \n",
       "6340  123715300  \n",
       "6341   82652600  \n",
       "6342  179521200  \n",
       "6343  105442900  \n",
       "6344  103529000  \n",
       "6345   91140200  \n",
       "\n",
       "[6346 rows x 7 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readTrain():\n",
    "    train = pd.read_csv(\"SPY.csv\")\n",
    "    return train\n",
    "\n",
    "readTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augFeatures(train):\n",
    "    train[\"Date\"] = pd.to_datetime(train[\"Date\"])\n",
    "    train[\"year\"] = train[\"Date\"].dt.year\n",
    "    train[\"month\"] = train[\"Date\"].dt.month\n",
    "    train[\"date\"] = train[\"Date\"].dt.day\n",
    "    train[\"day\"] = train[\"Date\"].dt.dayofweek\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train):\n",
    "    train = train.drop([\"Date\"], axis=1)\n",
    "    train_norm = train.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "    return train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastDay=30, futureDay=5):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureDay-pastDay):\n",
    "        X_train.append(np.array(train.iloc[i:i+pastDay]))\n",
    "        Y_train.append(np.array(train.iloc[i+pastDay:i+pastDay+futureDay][\"Adj Close\"]))\n",
    "    return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X,Y):\n",
    "    np.random.seed(10)\n",
    "    randomList = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randomList)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(X,Y,rate):\n",
    "    X_train = X[int(X.shape[0]*rate):]\n",
    "    Y_train = Y[int(Y.shape[0]*rate):]\n",
    "    X_val = X[:int(X.shape[0]*rate)]\n",
    "    Y_val = Y[:int(Y.shape[0]*rate)]\n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5680, 30, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read SPY.csv\n",
    "train = readTrain()\n",
    "\n",
    "# Augment the features (year, month, date, day)\n",
    "train_Aug = augFeatures(train)\n",
    "\n",
    "# Normalization\n",
    "train_norm = normalize(train_Aug)\n",
    "\n",
    "# build Data, use last 30 days to predict next 5 days\n",
    "X_train, Y_train = buildTrain(train_norm, 30, 5)\n",
    "\n",
    "# shuffle the data, and random seed is 10\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "# split training data and validation data\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "print(X_train.shape)\n",
    "# X_trian: (5710, 30, 10)\n",
    "# Y_train: (5710, 5, 1)\n",
    "# X_val: (634, 30, 10)\n",
    "# Y_val: (634, 5, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One to one model  \n",
    "input shape:  \n",
    "(5710, 1, 10)  \n",
    "output shape:  \n",
    "(5710, 1, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildOneToOneModel(shape):\n",
    "    #print(shape)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_length=shape[1], input_dim=shape[2],return_sequences=True))\n",
    "    # output shape: (1, 1)\n",
    "    model.add(TimeDistributed(Dense(1)))    # or use model.add(Dense(1))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5710, 1, 10)\n",
      "(5710, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab/anaconda3/envs/keras/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/ailab/anaconda3/envs/keras/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, return_sequences=True, input_shape=(1, 10))`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 1, 10)             840       \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 1, 1)              11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 1, 1)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,np.newaxis]\n",
    "Y_val = Y_val[:,np.newaxis]\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "model = buildOneToOneModel(X_train.shape)\n",
    "# callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "# model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many to one model  \n",
    "input shape:  \n",
    "(5710, 1, 10)  \n",
    "output shape:  \n",
    "(5710, 1, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildManyToOneModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_length=shape[1], input_dim=shape[2]))\n",
    "    # output shape: (1, 1)\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5684, 30, 10)\n",
      "(5684, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab/anaconda3/envs/keras/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/ailab/anaconda3/envs/keras/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, input_shape=(30, 10))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5684 samples, validate on 631 samples\n",
      "Epoch 1/1000\n",
      "5684/5684 [==============================] - 1s 252us/step - loss: 0.1084 - val_loss: 0.1774\n",
      "Epoch 2/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 0.0170 - val_loss: 0.0331\n",
      "Epoch 3/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 0.0030 - val_loss: 0.0114\n",
      "Epoch 4/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 0.0011 - val_loss: 0.0054\n",
      "Epoch 5/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 7.3759e-04 - val_loss: 0.0034\n",
      "Epoch 6/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 5.2878e-04 - val_loss: 0.0022\n",
      "Epoch 7/1000\n",
      "5684/5684 [==============================] - 1s 101us/step - loss: 4.0926e-04 - val_loss: 0.0015\n",
      "Epoch 8/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 3.3232e-04 - val_loss: 9.1358e-04\n",
      "Epoch 9/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 2.7934e-04 - val_loss: 8.1866e-04\n",
      "Epoch 10/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 2.4083e-04 - val_loss: 6.8411e-04\n",
      "Epoch 11/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 2.1137e-04 - val_loss: 6.4577e-04\n",
      "Epoch 12/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 1.8844e-04 - val_loss: 4.6683e-04\n",
      "Epoch 13/1000\n",
      "5684/5684 [==============================] - 1s 129us/step - loss: 1.6987e-04 - val_loss: 3.7273e-04\n",
      "Epoch 14/1000\n",
      "5684/5684 [==============================] - 1s 126us/step - loss: 1.5300e-04 - val_loss: 3.3537e-04\n",
      "Epoch 15/1000\n",
      "5684/5684 [==============================] - 1s 122us/step - loss: 1.3989e-04 - val_loss: 2.8689e-04\n",
      "Epoch 16/1000\n",
      "5684/5684 [==============================] - 1s 117us/step - loss: 1.2987e-04 - val_loss: 2.4996e-04\n",
      "Epoch 17/1000\n",
      "5684/5684 [==============================] - 1s 146us/step - loss: 1.2132e-04 - val_loss: 2.1384e-04\n",
      "Epoch 18/1000\n",
      "5684/5684 [==============================] - 1s 114us/step - loss: 1.1109e-04 - val_loss: 2.8164e-04\n",
      "Epoch 19/1000\n",
      "5684/5684 [==============================] - 1s 112us/step - loss: 1.0456e-04 - val_loss: 2.1344e-04\n",
      "Epoch 20/1000\n",
      "5684/5684 [==============================] - 1s 101us/step - loss: 9.7673e-05 - val_loss: 1.5941e-04\n",
      "Epoch 21/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 9.2293e-05 - val_loss: 1.7740e-04\n",
      "Epoch 22/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 8.8187e-05 - val_loss: 1.6267e-04\n",
      "Epoch 23/1000\n",
      "5684/5684 [==============================] - 1s 119us/step - loss: 8.3765e-05 - val_loss: 1.7258e-04\n",
      "Epoch 24/1000\n",
      "5684/5684 [==============================] - 1s 119us/step - loss: 8.0240e-05 - val_loss: 1.6430e-04\n",
      "Epoch 25/1000\n",
      "5684/5684 [==============================] - 1s 103us/step - loss: 7.7494e-05 - val_loss: 1.5605e-04\n",
      "Epoch 26/1000\n",
      "5684/5684 [==============================] - 1s 101us/step - loss: 7.5114e-05 - val_loss: 1.5099e-04\n",
      "Epoch 27/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 7.2244e-05 - val_loss: 1.4746e-04\n",
      "Epoch 28/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 7.0213e-05 - val_loss: 1.5266e-04\n",
      "Epoch 29/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 6.8701e-05 - val_loss: 1.3865e-04\n",
      "Epoch 30/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 6.7061e-05 - val_loss: 1.8152e-04\n",
      "Epoch 31/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 6.5889e-05 - val_loss: 1.4804e-04\n",
      "Epoch 32/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 6.4222e-05 - val_loss: 1.1768e-04\n",
      "Epoch 33/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 6.3630e-05 - val_loss: 1.0646e-04\n",
      "Epoch 34/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 6.2169e-05 - val_loss: 1.3188e-04\n",
      "Epoch 35/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 6.1398e-05 - val_loss: 1.1678e-04\n",
      "Epoch 36/1000\n",
      "5684/5684 [==============================] - 1s 103us/step - loss: 6.0571e-05 - val_loss: 1.1785e-04\n",
      "Epoch 37/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 5.9649e-05 - val_loss: 7.9820e-05\n",
      "Epoch 38/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 5.8435e-05 - val_loss: 8.2557e-05\n",
      "Epoch 39/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 5.7621e-05 - val_loss: 1.2181e-04\n",
      "Epoch 40/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 5.6913e-05 - val_loss: 1.1037e-04\n",
      "Epoch 41/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 5.6850e-05 - val_loss: 8.5581e-05\n",
      "Epoch 42/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 5.5631e-05 - val_loss: 1.1375e-04\n",
      "Epoch 43/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 5.6094e-05 - val_loss: 1.0779e-04\n",
      "Epoch 44/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 5.4003e-05 - val_loss: 7.8795e-05\n",
      "Epoch 45/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 5.3577e-05 - val_loss: 1.1234e-04\n",
      "Epoch 46/1000\n",
      "5684/5684 [==============================] - 1s 101us/step - loss: 5.3015e-05 - val_loss: 7.8077e-05\n",
      "Epoch 47/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 5.2549e-05 - val_loss: 7.0036e-05\n",
      "Epoch 48/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 5.1940e-05 - val_loss: 8.2518e-05\n",
      "Epoch 49/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 5.1839e-05 - val_loss: 5.7883e-05\n",
      "Epoch 50/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 5.1051e-05 - val_loss: 5.2286e-05\n",
      "Epoch 51/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 5.0287e-05 - val_loss: 8.0357e-05\n",
      "Epoch 52/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 5.0265e-05 - val_loss: 1.0192e-04\n",
      "Epoch 53/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 4.9797e-05 - val_loss: 6.2225e-05\n",
      "Epoch 54/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 4.9890e-05 - val_loss: 6.3281e-05\n",
      "Epoch 55/1000\n",
      "5684/5684 [==============================] - 1s 101us/step - loss: 4.8966e-05 - val_loss: 7.2497e-05\n",
      "Epoch 56/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 4.8349e-05 - val_loss: 5.9647e-05\n",
      "Epoch 57/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 4.8513e-05 - val_loss: 5.8054e-05\n",
      "Epoch 58/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 4.8247e-05 - val_loss: 3.7569e-05\n",
      "Epoch 59/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 4.7418e-05 - val_loss: 3.6322e-05\n",
      "Epoch 60/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 4.6376e-05 - val_loss: 4.0339e-05\n",
      "Epoch 61/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 4.6684e-05 - val_loss: 4.9758e-05\n",
      "Epoch 62/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 4.5605e-05 - val_loss: 5.5086e-05\n",
      "Epoch 63/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 4.7354e-05 - val_loss: 3.8657e-05\n",
      "Epoch 64/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 4.6600e-05 - val_loss: 2.8476e-05\n",
      "Epoch 65/1000\n",
      "5684/5684 [==============================] - 1s 102us/step - loss: 4.5205e-05 - val_loss: 4.5850e-05\n",
      "Epoch 66/1000\n",
      "5684/5684 [==============================] - 1s 103us/step - loss: 4.5390e-05 - val_loss: 3.5573e-05\n",
      "Epoch 67/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5684/5684 [==============================] - 1s 92us/step - loss: 4.5646e-05 - val_loss: 6.5030e-05\n",
      "Epoch 68/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 4.4726e-05 - val_loss: 5.3845e-05\n",
      "Epoch 69/1000\n",
      "5684/5684 [==============================] - 0s 86us/step - loss: 4.3479e-05 - val_loss: 4.1608e-05\n",
      "Epoch 70/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 4.3938e-05 - val_loss: 2.7577e-05\n",
      "Epoch 71/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 4.4320e-05 - val_loss: 5.7346e-05\n",
      "Epoch 72/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 4.4202e-05 - val_loss: 3.0143e-05\n",
      "Epoch 73/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 4.2781e-05 - val_loss: 2.6544e-05\n",
      "Epoch 74/1000\n",
      "5684/5684 [==============================] - 1s 88us/step - loss: 4.3055e-05 - val_loss: 2.7011e-05\n",
      "Epoch 75/1000\n",
      "5684/5684 [==============================] - 0s 86us/step - loss: 4.1709e-05 - val_loss: 2.7246e-05\n",
      "Epoch 76/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 4.2499e-05 - val_loss: 5.7182e-05\n",
      "Epoch 77/1000\n",
      "5684/5684 [==============================] - 1s 112us/step - loss: 4.2179e-05 - val_loss: 2.2456e-05\n",
      "Epoch 78/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 4.3029e-05 - val_loss: 1.9159e-05\n",
      "Epoch 79/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 4.1326e-05 - val_loss: 2.2325e-05\n",
      "Epoch 80/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 4.2497e-05 - val_loss: 1.4737e-05\n",
      "Epoch 81/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 4.3293e-05 - val_loss: 2.7002e-05\n",
      "Epoch 82/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 4.1522e-05 - val_loss: 2.7182e-05\n",
      "Epoch 83/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 4.0557e-05 - val_loss: 1.7537e-05\n",
      "Epoch 84/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.9965e-05 - val_loss: 2.3530e-05\n",
      "Epoch 85/1000\n",
      "5684/5684 [==============================] - 1s 103us/step - loss: 4.0636e-05 - val_loss: 1.1152e-05\n",
      "Epoch 86/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 3.9755e-05 - val_loss: 2.0894e-05\n",
      "Epoch 87/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 4.0081e-05 - val_loss: 1.5857e-05\n",
      "Epoch 88/1000\n",
      "5684/5684 [==============================] - 1s 116us/step - loss: 4.1672e-05 - val_loss: 1.5629e-05\n",
      "Epoch 89/1000\n",
      "5684/5684 [==============================] - 1s 137us/step - loss: 3.8999e-05 - val_loss: 2.0019e-05\n",
      "Epoch 90/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 3.8634e-05 - val_loss: 2.7351e-05\n",
      "Epoch 91/1000\n",
      "5684/5684 [==============================] - 1s 115us/step - loss: 3.8743e-05 - val_loss: 2.0642e-05\n",
      "Epoch 92/1000\n",
      "5684/5684 [==============================] - 1s 109us/step - loss: 3.8946e-05 - val_loss: 1.8371e-05\n",
      "Epoch 93/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 3.8342e-05 - val_loss: 1.9448e-05\n",
      "Epoch 94/1000\n",
      "5684/5684 [==============================] - 1s 113us/step - loss: 4.0173e-05 - val_loss: 1.4269e-05\n",
      "Epoch 95/1000\n",
      "5684/5684 [==============================] - 1s 115us/step - loss: 3.7754e-05 - val_loss: 1.6009e-05\n",
      "Epoch 96/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.8436e-05 - val_loss: 2.0539e-05\n",
      "Epoch 97/1000\n",
      "5684/5684 [==============================] - 1s 101us/step - loss: 3.8556e-05 - val_loss: 2.2299e-05\n",
      "Epoch 98/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 3.8636e-05 - val_loss: 2.6134e-05\n",
      "Epoch 99/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 3.7490e-05 - val_loss: 6.1103e-06\n",
      "Epoch 100/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 3.8416e-05 - val_loss: 3.9537e-05\n",
      "Epoch 101/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 3.7249e-05 - val_loss: 1.3225e-05\n",
      "Epoch 102/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.7427e-05 - val_loss: 1.8426e-05\n",
      "Epoch 103/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 3.6130e-05 - val_loss: 7.7377e-06\n",
      "Epoch 104/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 3.6179e-05 - val_loss: 6.2330e-06\n",
      "Epoch 105/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 3.6333e-05 - val_loss: 1.8869e-05\n",
      "Epoch 106/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 3.7388e-05 - val_loss: 2.8602e-05\n",
      "Epoch 107/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.6730e-05 - val_loss: 2.0616e-05\n",
      "Epoch 108/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 3.5817e-05 - val_loss: 1.2477e-05\n",
      "Epoch 109/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 3.5674e-05 - val_loss: 9.2523e-06\n",
      "Epoch 110/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 3.5570e-05 - val_loss: 4.7922e-06\n",
      "Epoch 111/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 3.4872e-05 - val_loss: 5.9744e-06\n",
      "Epoch 112/1000\n",
      "5684/5684 [==============================] - 1s 102us/step - loss: 3.4868e-05 - val_loss: 1.4321e-05\n",
      "Epoch 113/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.5851e-05 - val_loss: 4.8878e-06\n",
      "Epoch 114/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 3.6449e-05 - val_loss: 1.6262e-05\n",
      "Epoch 115/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 3.5251e-05 - val_loss: 2.2635e-05\n",
      "Epoch 116/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.4079e-05 - val_loss: 6.0563e-06\n",
      "Epoch 117/1000\n",
      "5684/5684 [==============================] - 1s 88us/step - loss: 3.4476e-05 - val_loss: 4.6467e-06\n",
      "Epoch 118/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 3.4101e-05 - val_loss: 5.2159e-06\n",
      "Epoch 119/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 3.4876e-05 - val_loss: 7.0726e-06\n",
      "Epoch 120/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.4190e-05 - val_loss: 1.3506e-05\n",
      "Epoch 121/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.7964e-05 - val_loss: 1.6140e-05\n",
      "Epoch 122/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 3.4999e-05 - val_loss: 1.6105e-05\n",
      "Epoch 123/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 3.4680e-05 - val_loss: 1.2201e-05\n",
      "Epoch 124/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 3.3130e-05 - val_loss: 4.0192e-06\n",
      "Epoch 125/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.3481e-05 - val_loss: 6.7792e-06\n",
      "Epoch 126/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.3800e-05 - val_loss: 3.7383e-06\n",
      "Epoch 127/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 3.2962e-05 - val_loss: 3.1886e-05\n",
      "Epoch 128/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 3.3013e-05 - val_loss: 4.9780e-06\n",
      "Epoch 129/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 3.3908e-05 - val_loss: 4.3871e-06\n",
      "Epoch 130/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 3.2917e-05 - val_loss: 1.9040e-05\n",
      "Epoch 131/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 3.1941e-05 - val_loss: 1.5412e-05\n",
      "Epoch 132/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.2896e-05 - val_loss: 3.9837e-05\n",
      "Epoch 133/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 3.4398e-05 - val_loss: 1.0688e-05\n",
      "Epoch 134/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 3.3461e-05 - val_loss: 2.6759e-05\n",
      "Epoch 135/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.3126e-05 - val_loss: 1.1057e-05\n",
      "Epoch 136/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 3.2385e-05 - val_loss: 1.5917e-05\n",
      "Epoch 137/1000\n",
      "5684/5684 [==============================] - 1s 102us/step - loss: 3.1704e-05 - val_loss: 1.0788e-05\n",
      "Epoch 138/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5684/5684 [==============================] - 1s 122us/step - loss: 3.2466e-05 - val_loss: 8.9708e-06\n",
      "Epoch 139/1000\n",
      "5684/5684 [==============================] - 1s 117us/step - loss: 3.1310e-05 - val_loss: 5.3893e-06\n",
      "Epoch 140/1000\n",
      "5684/5684 [==============================] - 1s 120us/step - loss: 3.1377e-05 - val_loss: 7.8616e-06\n",
      "Epoch 141/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 3.4203e-05 - val_loss: 4.3101e-06\n",
      "Epoch 142/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 3.2004e-05 - val_loss: 1.9826e-05\n",
      "Epoch 143/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.1695e-05 - val_loss: 1.7044e-05\n",
      "Epoch 144/1000\n",
      "5684/5684 [==============================] - 1s 101us/step - loss: 3.1212e-05 - val_loss: 1.3673e-05\n",
      "Epoch 145/1000\n",
      "5684/5684 [==============================] - 1s 103us/step - loss: 3.2424e-05 - val_loss: 1.5416e-05\n",
      "Epoch 146/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 3.1357e-05 - val_loss: 1.2215e-05\n",
      "Epoch 147/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 3.0689e-05 - val_loss: 3.5724e-06\n",
      "Epoch 148/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 3.1404e-05 - val_loss: 2.0496e-05\n",
      "Epoch 149/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 3.0500e-05 - val_loss: 3.4274e-06\n",
      "Epoch 150/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 3.0888e-05 - val_loss: 3.6614e-06\n",
      "Epoch 151/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 3.1780e-05 - val_loss: 5.4702e-06\n",
      "Epoch 152/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.2976e-05 - val_loss: 3.4221e-06\n",
      "Epoch 153/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 3.1963e-05 - val_loss: 3.7326e-06\n",
      "Epoch 154/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 3.0788e-05 - val_loss: 2.7003e-06\n",
      "Epoch 155/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 3.0830e-05 - val_loss: 3.4418e-06\n",
      "Epoch 156/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 2.9850e-05 - val_loss: 2.7331e-06\n",
      "Epoch 157/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 2.9690e-05 - val_loss: 1.3497e-05\n",
      "Epoch 158/1000\n",
      "5684/5684 [==============================] - 1s 122us/step - loss: 3.0552e-05 - val_loss: 8.2893e-06\n",
      "Epoch 159/1000\n",
      "5684/5684 [==============================] - 1s 117us/step - loss: 2.9601e-05 - val_loss: 1.2594e-05\n",
      "Epoch 160/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 3.0350e-05 - val_loss: 3.6533e-06\n",
      "Epoch 161/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.9800e-05 - val_loss: 8.0120e-06\n",
      "Epoch 162/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.9792e-05 - val_loss: 7.6121e-06\n",
      "Epoch 163/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 2.9727e-05 - val_loss: 3.4769e-06\n",
      "Epoch 164/1000\n",
      "5684/5684 [==============================] - 1s 102us/step - loss: 3.0774e-05 - val_loss: 4.1568e-06\n",
      "Epoch 165/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.2727e-05 - val_loss: 3.1485e-06\n",
      "Epoch 166/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 3.0457e-05 - val_loss: 3.7520e-06\n",
      "Epoch 167/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 2.9924e-05 - val_loss: 1.6159e-05\n",
      "Epoch 168/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.1546e-05 - val_loss: 3.1955e-06\n",
      "Epoch 169/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 2.8817e-05 - val_loss: 1.1017e-05\n",
      "Epoch 170/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 2.8877e-05 - val_loss: 3.9668e-06\n",
      "Epoch 171/1000\n",
      "5684/5684 [==============================] - 1s 114us/step - loss: 2.9931e-05 - val_loss: 2.6095e-06\n",
      "Epoch 172/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 2.9740e-05 - val_loss: 2.9853e-06\n",
      "Epoch 173/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 3.0681e-05 - val_loss: 6.7807e-06\n",
      "Epoch 174/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 2.9436e-05 - val_loss: 2.8115e-06\n",
      "Epoch 175/1000\n",
      "5684/5684 [==============================] - 0s 84us/step - loss: 2.8382e-05 - val_loss: 8.3271e-06\n",
      "Epoch 176/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 2.8506e-05 - val_loss: 5.0531e-06\n",
      "Epoch 177/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 2.9001e-05 - val_loss: 1.7438e-05\n",
      "Epoch 178/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 2.8199e-05 - val_loss: 2.6581e-06\n",
      "Epoch 179/1000\n",
      "5684/5684 [==============================] - 0s 82us/step - loss: 2.8486e-05 - val_loss: 1.6272e-05\n",
      "Epoch 180/1000\n",
      "5684/5684 [==============================] - 1s 88us/step - loss: 2.9505e-05 - val_loss: 1.5110e-05\n",
      "Epoch 181/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 2.8423e-05 - val_loss: 2.6296e-05\n",
      "Epoch 182/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 3.0133e-05 - val_loss: 9.2916e-06\n",
      "Epoch 183/1000\n",
      "5684/5684 [==============================] - 0s 86us/step - loss: 2.9250e-05 - val_loss: 6.5459e-06\n",
      "Epoch 184/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 2.7891e-05 - val_loss: 1.6196e-05\n",
      "Epoch 185/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 2.8767e-05 - val_loss: 3.5777e-06\n",
      "Epoch 186/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 2.8459e-05 - val_loss: 4.8215e-06\n",
      "Epoch 187/1000\n",
      "5684/5684 [==============================] - 1s 176us/step - loss: 2.8357e-05 - val_loss: 3.3594e-06\n",
      "Epoch 188/1000\n",
      "5684/5684 [==============================] - 1s 176us/step - loss: 2.7815e-05 - val_loss: 1.9407e-05\n",
      "Epoch 189/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 2.8288e-05 - val_loss: 9.9357e-06\n",
      "Epoch 190/1000\n",
      "5684/5684 [==============================] - 0s 81us/step - loss: 2.7692e-05 - val_loss: 2.0350e-05\n",
      "Epoch 191/1000\n",
      "5684/5684 [==============================] - 1s 102us/step - loss: 2.7803e-05 - val_loss: 3.4769e-06\n",
      "Epoch 192/1000\n",
      "5684/5684 [==============================] - 1s 134us/step - loss: 2.7713e-05 - val_loss: 2.7175e-06\n",
      "Epoch 193/1000\n",
      "5684/5684 [==============================] - 0s 86us/step - loss: 2.7490e-05 - val_loss: 7.0146e-06\n",
      "Epoch 194/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 2.8235e-05 - val_loss: 2.1716e-05\n",
      "Epoch 195/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 2.9521e-05 - val_loss: 5.1886e-06\n",
      "Epoch 196/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 2.7811e-05 - val_loss: 5.9231e-06\n",
      "Epoch 197/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 2.9730e-05 - val_loss: 4.1554e-06\n",
      "Epoch 198/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 2.7892e-05 - val_loss: 2.2521e-06\n",
      "Epoch 199/1000\n",
      "5684/5684 [==============================] - 1s 113us/step - loss: 2.8067e-05 - val_loss: 1.9241e-05\n",
      "Epoch 200/1000\n",
      "5684/5684 [==============================] - 1s 143us/step - loss: 2.8219e-05 - val_loss: 7.5342e-06\n",
      "Epoch 201/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 2.9364e-05 - val_loss: 6.4923e-06\n",
      "Epoch 202/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 2.7913e-05 - val_loss: 2.3357e-06\n",
      "Epoch 203/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 2.8447e-05 - val_loss: 6.5212e-06\n",
      "Epoch 00203: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f165b672fd0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 30, 1)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "# because no return sequence, Y_train and Y_val shape must be 2 dimension\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "model = buildManyToOneModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
